## 0. 缘起
在蟒营学习在课程结束之后想要做开发一个项目~,也就是将个人石墨文档中的内容爬取出来得到所有文件的文件名&链接以及所在的层级结构;以便于对自己混乱的石墨文档中的所以文件有一个全局的了解~  

之前的思考~copy到下面~ 
## 想要开发的内容      
1. 把石墨内文档的文件形成目录的形式并且带有链接给爬出来
再次优化:
2. 将幕布文档中所有文档的链接给爬出来 
### 产品名
石墨文件树| Shimo file-tree |Sft

### 1. 核心需求  
用户可以登录自己的石墨账户,并且把一个文件夹中的所有石墨文档的名字跟链接 爬去并且保存到一个本地的文档中  

### 2. 需求拆解成设计 


## 问题 
1. 开发应该分哪几个阶段?  
    ![开发过程](http://slides.101.camp/ch11py101camp.html#/2/10)
    设计:开发:测试:运营=4:1:1:4
2. 每个阶段最核心的问题是什么? 
3. 每个问题已知主流方案是什么? 
    - 哪个最好? 为什么? 
    - 自己选择哪个? 为什么? 
4. 如何判断一个网站可以爬下来? 


**其他问题**    
1. 如果开始我第一个应该解决的问题是什么? 
2. 如果要协作开始,我第一个要解决的问题是什么呢? 
3. 如何协作, 任务应该如何分配&应该如何讨论呢?  
4. 为了实现MVP的前进应该怎么设计关卡? 
    - 爬取百度首页
### 爬虫知识点   
> 该部分非核心问题,不要将自己沉入到问题当中  

1. 爬山的基本原理是什么?
2. 爬虫有哪些框架? 
    - Python自带的库是什么?  局限性是什么?
        - urllib urlopen
    - 都有哪些框架,库? 各自的 `优缺点`都是什么? 
    - 那个框架符合我的需求?  

爬虫相关库:   


- Beautiful Soup。名气大，整合了一些常用爬虫需求。缺点：不能加载JS。
- Scrapy。看起来很强大的爬虫框架，可以满足简单的页面爬取（比如可以明确获知url pattern的情况）。用这个框架可以轻松爬下来如亚马逊商品信息之类的数据。但是对于稍微复杂一点的页面，如weibo的页面信息，这个框架就满足不了需求了。    
- mechanize。优点：可以加载JS。缺点：文档严重缺失。不过通过官方的example以及人肉尝试的方法，还是勉强能用的。    
- selenium。这是一个调用浏览器的driver，通过这个库你可以直接调用浏览器完成某些操作，比如输入验证码。   
- cola。一个分布式爬虫框架。项目整体设计有点糟，模块间耦合度较高，不过值得借鉴。  

以下是我的一些实践经验：  
- 对于简单的需求，比如有固定pattern的信息，怎么搞都是可以的。  
- 对于较为复杂的需求，比如爬取动态页面、涉及状态转换、涉及反爬虫机制、涉及高并发，这种情况下是很难找到一个契合需求的库的，很多东西只能自己写。


>[用Python写爬虫，用什么方式、框架比较好？ - 知乎](https://www.zhihu.com/question/19899608)

190318  15:07:25 -  
190404 11:13:32 重启项目开始研究




## logging
190406 10:40:36 - 10:43:14 